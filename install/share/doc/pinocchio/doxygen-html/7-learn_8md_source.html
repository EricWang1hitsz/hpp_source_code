<!-- HTML header for doxygen 1.8.11-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>pinocchio: 7-learn.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="pinocchio.ico" rel="icon" type="image/x-icon">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/SVG"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="pinocchio.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">pinocchio
   &#160;<span id="projectnumber">2.1.3</span>
   </div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('7-learn_8md.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">7-learn.md</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# 7) Learning to flight (aka policy learning)</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;## Objective</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;The objective of this tutorial is to study how to directly solve an</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;optimal control problem, either by computing a trajectory from current</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;state to goal, or by computing a policy. To keep decent computation</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;timings with simple python code, we will only work with a simple</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;inverted pendulum with limited torque, that must swing to raise to</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;standing configuration. The presented algorithms successively compute an</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;optimal trajectory, a discretized policy with a Q-table and with a</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;linear network, and a continuous policy with a deep neural network.</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;</div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;## 7.0) prerequesites</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;We need a pendulum model and a neural network.</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;### Prerequesite 1</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;Inverted pendulum model</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;Two models are provided. The first one is continuous and is implemented</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;with Pinocchio and is available in [pendulum.py](pendulum_8py_source.html)</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;with class `Pendulum`. The</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;code is generic for a N-pendulum. We will use the 1-dof model. The state</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;is `[q, v]` the angle and angular velocity of the pendulum. The control</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;is the joint torque. The pendulum weights 1kg, measures 1m with COM at</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;0.5m of the joint. Do not forget to start `gepetto-gui` before</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;rendering the model. Each time the simulator is integrated, it returns a</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;new state and the reward for the previous action (implement to be the</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;weighted sum of squared position, velocity and control). The state is</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;recorded as a member of the class and can be accessed through env.x. Do</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;not forget to copy it before modifying it.</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;```py</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;from pendulum import Pendulum</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;from pinocchio.utils import *</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;env = Pendulum(1)  # Continuous pendulum</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;NX = env.nobs      # ... training converges with q,qdot with 2x more neurones.</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;NU = env.nu        # Control is dim-1: joint torque</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;x = env.reset()    # Sample an initial state</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;u = rand(NU)       # Sample a control</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;x, reward = env.step(u)   # Integrate simulator for control u and get reward.</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;env.render()       # Display model at state env.x</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;```</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;A second version of the same model is provided with a discrete dynamics,</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;in [dpendulum.py](dpendulum_8py_source.html). The state is again `[q, v]`, however discretized in NQ</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;position and NV velocity. The state is then a integer equal to iq*NV+iv,</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;ranging from 0 to NQ*NV=NX. Controls are also discretized from 0 to NU.</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;```py</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;from dpendulum import DPendulum</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;env = DPendulum()</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;NX = env.nx  # Number of (discrete) states</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;NU = env.nu  # Number of (discrete) controls</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;env.reset()</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;env.step(5)</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;env.render()</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;```</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;Other models could be used. In particular, we used a similar API to the</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;Gym from OpenAI, that you might be interested to browsed and possibly</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;try with the following algorithms.</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;### Prerequesite 2</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;A neural network with optimizers</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;We will use the Tensor Flow from Google, available thanks to pip.</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;```</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;pip install --user tensorflow tflearn</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;```</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;## 7.1) Optimizing an optimal trajectory</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;For the first tutorial, we implement a nonlinear optimization program</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;optimizing the cost of a single trajectory for the continious pendulum.</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;The trajectory is represented by its initial state x0 and the vector of</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;piecewise-constant control `U=[u0 ... uT-1]`, with `T` the number of</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;timestep. The cost is simply the integral of the cost function l(x,u)</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;returned by the pendulum environment.</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;Then the integral cost is optimized starting from a 0 control</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;trajectory, until the pendulum finally reaches a standing state. Observe</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;that the number of swings is possibly sub-optimal.</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;The code to optimize the trajectory is available in [ocp.py](ocp_8py_source.html).</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;## 7.2) Q-table resolution for discrete pendulum</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;We now consider the discrete model, with NX state and NU control.</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;Imagine this model as a chess board with a maze (possibly nonplanar)</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;drawn on it, and you ask the system to discover a path from an inital</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;state to the final state at the center of the board. When performing a</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;trial, the system is informed of success or failure by receiving reward</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;1 when reaching the goal, or otherwise 0 after 100 moves. To record the</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;already-explored path, we can stored a table of NX per NU values, each</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;giving how likely we would be rewarded if taking action U at state X.</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;This table is named the Q-table, and corresponds to the Hamiltonian</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;(Q-value) of the discrete system. The Q-values can be back-propagated</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;along the table using the Dijkstra algorithm. Since we do not now the</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;goal(s) states, the back propagation is done along random roll-outs</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;inside the maze, which likely converges to an approximation of the exact</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;Hamiltonian. Once the Q-table is computed, the optimal policy is simply</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;chosen by maximizing the vector of Q-values corresponding to the row of</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;state X.</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;This algorithm is available in the file [qtable.py](qtable_8py_source.html).</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;## 7.3) Q-table using a linear net</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;The idea is to similarly approximate the Q-value for the continuous</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;model. Since the continious model has both infinitely many states and</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;controls, a table can not make it. We will rather use any function basis</div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;to approximate the Q-value. For the tutorial, we have chosen to use a</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;deep neural net. Firt, let&#39;s use a simple net for storing the Q-table.</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;</div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;Basically, the vectory of Q-values for all possible control u is</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;obtained by multiplying the Q-table by a one-hot vector (0 everywhere</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;except a single 1) corresponding to the state. The optimal policy is</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;then the maximum of this vector: `iu^* = argmax(Q*h(ix))`, with `h(ix)</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;= [ 0 0 ... 0 1 0 ... 0]`, ix and iu being indexes of both state and</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;control. We use tensor flow to store array Q. The Q-value net is simply</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;the multiplication of Q by one-hot x, and the policy the argmax of the</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;result.</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;Now, the coefficients of Q are the parameters defining the Q-value (and</div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;then the policy). They must be optimized to fit the cost function. From</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;Hamiltion-Jacobi-Belman equation, we know that Q(x,u) = l(x,u) + max\_u2</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;Q(f(x,u),u2). We optimize the Q value so that this residual is minimized</div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;along the samples collected from successive roll-outs inside the maze.</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;</div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;The implementation of this algorithm is available in [qnet.py](qnet_8py_source.html).</div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;Observe that the convergence is not as fast as with the Q-Table algorithm.</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;## 7.4) Actor-critic network</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;We will now optimize a continuous policy and the corresponding</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;Q-function, using an &quot;Actor-Critic&quot; method proposed in [&quot;Continuous</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;control with deep reinforcement learning&quot;, by Lillicrap et al,</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;arXiv:1509.02971](https://arxiv.org/abs/1509.02971).</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;</div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;Two networks are used to represent the Q function and the policy. The</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;first network has two inputs: state x and control u. Its outpout is a</div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;scalar. It is optimized to minimize the residual corresponding to HJB</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;equation along a batch of sample points collected along previous</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;roll-outs.</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;The policy function has a single input: state X. Its output is a control</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;vector U (dimension 1 for the pendulum). It is optimize to maximize the</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;Q function , i.e at each state, Pi(x) corresponds to the maximum over</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;all possible controls u of Q(x,u).</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;Two critical aspects are reported in the paper and implemented in the</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;tutorial. First, we learn over a batch of random samples collected from</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;many previous roll-outs, in order to break the temporal dependancy in</div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;the batch. Second, we regularize the optimization of both Q-value</div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;(critic) and policy (actor) networks by storing a copy of both network,</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;and only slightly modifying these copy at each steps.</div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;The corresponding algorithm is implemented in the file [continuous.py](continuous_8py_source.html)</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;The training phase requires 100 roll-outs and some</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;minutes (maybe more on a virual machine).</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;## 7.5) Training the network with the OCP solver</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;Using the OCP solver, you might compute a few optimal trajectories (say</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;10) starting from various initial conditions. Initialize the replay</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;memory with the 10x50 points composing the 10 optimal trajectories and</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;optimize the network from these replay memory only (without additional</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;roll-outs, but using the same small-size batch). Play with the learning</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;parameters until the network converges.</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;When properly implemented, the OCP produces better accuracy than the</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;policy. However, at run-time, the policy is much cheaper to evaluate</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;than solving a new OCP. I am currently considering how to use the</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;network to warm-start or guide the OCP solver at run-time.</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;</div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;The provided solvers (trajectory and policy) runs reasonably well for</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;the 1-pendulum. It is more difficult to tune for a more-complex</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;dynamics, such as a 2-pendulum. You may want to try on a quadcopter</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;robot (hence the title of the tutorial) but I except it to be a serious</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;job.</div></div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><b>7-learn.md</b></li>
    <li class="footer">Generated on Tue Aug 20 2019 11:12:35 for pinocchio by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.11 </li>
  </ul>
</div>
</body>
</html>
